"""StackGAN_v2."""

# pylint: disable=no-member

import torch
import torch.nn as nn
import torch.nn.functional as F

from stackgan.utils.submodules import (
    RootDecoder, NodeDecoder, Generator, Discriminator,
    ConditioningAugmentation,
)

class StackGAN_v2(nn.Module):  # v2 is ackward without underscore... pylint: disable=invalid-name
    """StackGAN_v2.

    StackGAN_v2 consists of a main Generator submodule and multiple
    other generators that intercept hidden representations of the main
    Generator to generate the image at different scales. Each is assigned
    its own discriminator. This implementation has 3 generators and
    dscriminators, and the images generated as 64x64, 128x128, 256x256.
    Can optionally use Auxiliary classifier.

    Attributes:
        decoders(nn.ModuleList): contains the decoders of
            generator module, from smaller to larger scales.
        generators(nn.ModuleList): contains the final layer
            of the generators, from smaller to larger scales.
        discriminators(nn.ModuleList): contains the
            discriminators, from smaller to larger scales.
    """

    def __init__(self, Ng, Nd, cond_dim, noise_dim, n_class=None, reverse=False):
        """Init.

        Args:
            Ng(int): N_g parameter in StackGAN++ paper, channels
                of final representation of decoder module.
            Nd(int): N_d parameter in StackGAN++ paper, channels of first
                representation of discriminators.
            cond_dim(int): dimension of conditioning variable.
            noise_dim(int): dimension of noise.
            n_class(int|None): number of output classes for auxiliary
                classifier, default=`None` (no auxiliary classifier).
            reverse(bool, optional): whether to reverse arg order in
                concatenation of representation and cond var,
                default=`False`.
        """

        super().__init__()

        self.decoders = nn.ModuleList([
            RootDecoder(64 * Ng, cond_dim, noise_dim),
            NodeDecoder(4 * Ng, cond_dim, reverse=reverse),
            NodeDecoder(2 * Ng, cond_dim, reverse=reverse),
        ])

        self.generators = nn.ModuleList([
            Generator(4 * Ng),
            Generator(2 * Ng),
            Generator(Ng),
        ])

        self.discriminators = nn.ModuleList([
            Discriminator(Nd, 64, cond_dim, n_class, reverse=reverse),
            Discriminator(Nd, 128, cond_dim, n_class, reverse=reverse),
            Discriminator(Nd, 256, cond_dim, n_class, reverse=reverse),
        ])

        self.cond_aug = ConditioningAugmentation(cond_dim, 1024)

    def forward(self, emb, noise):  # pylint: disable=arguments-differ
        """Forward prop.

        Forward propagation through all the generator and the discriminators
        of the StackGAN_v2.

        Args:
            emb(torch.Tensor): caption embedding
            noise(torch.Tensor): random noise.

        Returns:
            (From smaller to larger scales) a list of generated
            torch.Tensor images, a list of intermediate torch.Tensor
            representations, the mean and the standard deviation generated by
            the Conditioning Augmentation.
        """

        cond_var, mus, stds = self.cond_aug(emb)

        inter_repr = self.decoders[0](cond_var, noise)
        img = self.generators[0](inter_repr)

        inter_reprs = [inter_repr]
        gen_imgs = [img]

        for dec, gen in zip(self.decoders[1:], self.generators[1:]):
            inter_repr = dec(inter_repr, cond_var)
            img = gen(inter_repr)

            inter_reprs.append(inter_repr)
            gen_imgs.append(img)

        return gen_imgs, inter_reprs, mus, stds

    def discr_fake_images(self, imgs, emb=None, mus=None):
        """Process generated images and their corresponding caption
        with the discriminators.

        Args:
            imgs(list of torch.Tensors): images generated at
                different scales, from small to large.
            emb(torch.Tensor, optional): caption embedding,
                default=`None`. Note that either this or mus
                must be provided. Overwrites mus if passed.
            mus(torch.tensor, optional): calculated mean of
                Conditioning Augmentation. Note that either
                this or emb must be provided.

        Returns:
            torch.Tensor with probabilities of size (3, 2, batch_size),
            from smaller to larger scales. If auxiliary classifier, also
            returns torch.Tensor of logits of size (3, batch_size, n_class).

        Raises:
            AssertionError: neither emb nor mus were provided.
        """

        assert emb is not None or mus is not None

        if emb is not None:
            _, mus, _ = self.cond_aug(emb)

        mus = mus.detach()  # free graph

        discr_probs = [discr(img, mus) for discr, img in
                       zip(self.discriminators, imgs)]

        try:
            return torch.stack(discr_probs)
        except TypeError:  # tuple instead of tensor
            return (torch.stack([discr_out[0] for discr_out in discr_probs]),
                    torch.stack([discr_out[1] for discr_out in discr_probs]))

    def discr_real_images(self, img, emb=None, mus=None):
        """Process real images and real caption with the discriminators.

        Args:
            img(torch.Tensor): image of size (batch_size, 3, 256, 256).
            emb(torch.Tensor, optional): caption embedding,
                default=`None`. Note that either this or mus
                must be provided. Overwrites mus if passed.
            mus(torch.tensor, optional): calculated mean of
                Conditioning Augmentation. Note that either
                this or emb must be provided.

        Returns:
            torch.Tensor with probabilities of size (3, 2, batch_size),
            from smaller to larger scales. If auxiliary classifier, also
            returns torch.Tensor of logits of size (3, batch_size, n_class).

        Raises:
            AssertionError: neither emb nor mus were provided.
        """

        d1_img = F.interpolate(img, size=64)
        d2_img = F.interpolate(img, size=128)
        imgs = [d1_img, d2_img, img]

        return self.discr_fake_images(imgs, emb, mus)

    def generate(self, emb, noise):
        """Generate images given embedding and noise.

        Args:
            emb(torch.Tensor): caption embedding.
            noise(torch.Tensor): noise vector.

        Returns:
            List of torch.Tensor images of all scales,
            from smallest to largest.
        """

        images = []

        if self.training:
            cond_var, _, _ = self.cond_aug(emb)
        else:
            _, cond_var, _ = self.cond_aug(emb)
        inter_repr = self.decoders[0](cond_var, noise)
        images.append(self.generators[0](inter_repr))
        for dec, gen in zip(self.decoders[1:], self.generators[1:]):
            inter_repr = dec(inter_repr, cond_var)
            images.append(gen(inter_repr))

        return images

    def sample_generator(self, layer=3, include_img_generator=True):
        """Creates the sample generator of choice.

        Retrieves the necessary modules for the requested
        generator where the synthetic samples are expected
        to come from.

        Args:
            layer(int): Up to which layer to retrieve from
                decoders.
            include_img_generator(bool, optional): whether
                to include the corresponding Generator
                yielding an image from the representation
                of the decoder, default=`True`.

        Returns:
            the requested instance from the
            SampleGenerator class.
        """
        generator = SampleGenerator(self.cond_aug, self.decoders[:layer],
                                    self.generators[layer-1] if include_img_generator else None)

        return generator

class SampleGenerator(nn.Module):
    """Auxiliary Module for sample generation
    based on StackGAN_v2 modules.

    Mainly used to be consistent with StachGAN arguments
    and avoid problems with nn.Sequential where we
    cannot use tuple inputs etc.

    Attributes:
        cond_aug(nn.Module): ConditioningAugmentation module.
        decoders(nn.ModuleList): Decoders' list from
            StackGAN_v2.
        generator(nn.Module|None): Image generator.
    """

    def __init__(self, cond_aug, stackgan_decoder, stackgan_generator=None):
        """Init.

        Args:
            cond_aug(nn.Module): ConditioningAugmentation module.
            stackgan_decoders(nn.ModuleList): Decoders' list from
                StackGAN_v2.
            stackgan_generator(nn.Module|None): Image generator.
        """

        super().__init__()
        self.cond_aug = cond_aug
        self.decoders = stackgan_decoder
        self.generator = stackgan_generator

    def forward(self, emb, noise):  # pylint: disable=arguments-differ
        """Forward propagation.

        Meant to resemble as much as possible the
        StackGAN_v2 forward method.

        Args:
            emb(torch.Tensor): caption embedding
            noise(torch.Tensor): random noise.

        Returns:
            Resulting sample, if generator is used
                then that is an image, else a
                representation.
        """

        if self.training:
            cond_var, mus, stds = self.cond_aug(emb)
        else:
            _, cond_var, _ = self.cond_aug(emb)

        reprs = self.decoders[0](cond_var, noise)
        for dec in self.decoders[1:]:
            reprs = dec(reprs, cond_var)

        if self.training:
            if self.generator is not None:
                return self.generator(reprs), mus, stds
            return reprs, mus, stds
        # else
        if self.generator is not None:
            return self.generator(reprs)
        return reprs
